{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b957d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AriZu\\.conda\\envs\\train\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 用于“分库分测、禁止泄露” & “索引全段落”\n",
    "from datasets import load_dataset\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from config import settings\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f61aa775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测不到已保存的向量库，开始构建...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 87599/87599 [00:00<00:00, 880190.01 examples/s]\n",
      "Generating validation split: 100%|██████████| 10570/10570 [00:00<00:00, 1003072.39 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 Train split 构建索引，共 238 段落。\n",
      "向量库已保存：embeddings/rag_train_full_paragraphs\n",
      "Hit@10: 0.3460, MRR@10: 0.2260\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def build_index():\n",
    "    \"\"\"\n",
    "    构建向量库：仅使用 SQuAD train split 的整段 context\n",
    "    \"\"\"\n",
    "    train_ds = load_dataset(\"squad\", split=\"train[:2%]\")\n",
    "    seen, contexts = set(), []\n",
    "    for item in train_ds:\n",
    "        ctx = item[\"context\"]\n",
    "        if ctx not in seen:\n",
    "            seen.add(ctx)\n",
    "            contexts.append(ctx)\n",
    "    print(f\"使用 Train split 构建索引，共 {len(contexts)} 段落。\")\n",
    "\n",
    "    docs = [Document(page_content=ctx, metadata={\"source\": f\"train_paragraph_{i}\"})\n",
    "            for i, ctx in enumerate(contexts)]\n",
    "    embeddings = OpenAIEmbeddings(model=settings.EMBEDDING_MODEL)\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    os.makedirs(\"embeddings/rag_train_full_paragraphs\", exist_ok=True)\n",
    "    db.save_local(\"embeddings/rag_train_full_paragraphs\")\n",
    "    print(\"向量库已保存：embeddings/rag_train_full_paragraphs\")\n",
    "    return db\n",
    "\n",
    "\n",
    "def evaluate_retrieval(dev_ds, db, top_k=10):\n",
    "    \"\"\"\n",
    "    基于答案文本评估检索效果（Hit@k & MRR@k），而非简单匹配完整 context。\n",
    "    \"\"\"\n",
    "    hits, rr_list = [], []\n",
    "    for item in dev_ds:\n",
    "        query = item[\"question\"]\n",
    "        answers = item.get(\"answers\", {}).get(\"text\", [])\n",
    "\n",
    "        retrieved = db.similarity_search(query, k=top_k)\n",
    "        hit = 0\n",
    "        rr = 0\n",
    "        for rank, doc in enumerate(retrieved):\n",
    "            content = doc.page_content.lower()\n",
    "            if any(ans.lower() in content for ans in answers):\n",
    "                hit = 1\n",
    "                if rr == 0:\n",
    "                    rr = 1.0 / (rank + 1)\n",
    "        hits.append(hit)\n",
    "        rr_list.append(rr)\n",
    "    return np.mean(hits), np.mean(rr_list)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    index_path = \"embeddings/rag_train_full_paragraphs\"\n",
    "    # 若向量库不存在，则重新构建\n",
    "    if not os.path.isdir(index_path) or not os.path.exists(os.path.join(index_path, \"index.faiss\")):\n",
    "        print(\"检测不到已保存的向量库，开始构建...\")\n",
    "        db = build_index()\n",
    "    else:\n",
    "        embeddings = OpenAIEmbeddings(model=settings.EMBEDDING_MODEL)\n",
    "        db = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "        print(\"已加载已有向量库：\", index_path)\n",
    "\n",
    "    dev_ds = load_dataset(\"squad\", split=\"validation[:2%]\")\n",
    "    hit10, mrr10 = evaluate_retrieval(dev_ds, db, top_k=10)\n",
    "    print(f\"Hit@10: {hit10:.4f}, MRR@10: {mrr10:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
