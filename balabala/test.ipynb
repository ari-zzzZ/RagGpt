{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62764137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AriZu\\.conda\\envs\\train\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# rag_eval_ms_marco_v1_1.py\n",
    "# 基于 MS MARCO Passage Ranking v1.1 数据集 测试检索 Hit@k & MRR@k\n",
    "# 数据量约 168.7 MB（压缩），train=82k queries，validation=10k queries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from config import settings\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "INDEX_DIR = \"embeddings/ms_marco_v1_1_passages\"\n",
    "TOP_K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d39da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index_with_speed(batch_size=200, cache_dir=None):\n",
    "    \"\"\"构建向量库并显示当前嵌入速率，支持断点续传\"\"\"\n",
    "    print(\"Loading MS MARCO v1.1 train split passages...\")\n",
    "    train_ds = load_dataset(\"ms_marco\", \"v1.1\", split=\"train\", cache_dir=cache_dir)\n",
    "\n",
    "    # 使用字典而非集合来处理去重，同时保存ID信息\n",
    "    passage_dict = {}\n",
    "    for item in tqdm(train_ds, desc=\"Collecting passages\"):\n",
    "        p = item[\"passages\"]\n",
    "        for i, text in enumerate(p[\"passage_text\"]):\n",
    "            if text not in passage_dict:\n",
    "                passage_dict[text] = f\"msmarco_passage_{len(passage_dict)}\"\n",
    "    \n",
    "    passages = list(passage_dict.keys())\n",
    "    ids = list(passage_dict.values())\n",
    "    print(f\"Collected {len(passages)} unique passages.\")\n",
    "    \n",
    "    # 确保目录存在\n",
    "    os.makedirs(INDEX_DIR, exist_ok=True)\n",
    "    \n",
    "    # 实现断点续传机制\n",
    "    checkpoint_file = os.path.join(INDEX_DIR, \"embedding_checkpoint.npz\")\n",
    "    start_idx = 0\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # 检查检查点文件是否存在\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        try:\n",
    "            checkpoint = np.load(checkpoint_file, allow_pickle=True)\n",
    "            all_embeddings = checkpoint['embeddings'].tolist()\n",
    "            start_idx = len(all_embeddings)\n",
    "            print(f\"Resuming from checkpoint with {start_idx} embeddings\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}. Starting from scratch.\")\n",
    "            all_embeddings = []\n",
    "            start_idx = 0\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "    \n",
    "    # 按批次进行嵌入\n",
    "    remaining = passages[start_idx:]\n",
    "    batches = [remaining[i:i+batch_size] for i in range(0, len(remaining), batch_size)]\n",
    "    \n",
    "    embedder = OpenAIEmbeddings(model=settings.EMBEDDING_MODEL)\n",
    "\n",
    "    start = time.time()\n",
    "    pbar = tqdm(batches, desc=\"Embedding batches\")\n",
    "    try:\n",
    "        for i, batch in enumerate(pbar):\n",
    "            # 添加重试逻辑\n",
    "            max_retries = 3\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    embs = embedder.embed_documents(batch)\n",
    "                    all_embeddings.extend(embs)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt < max_retries - 1:\n",
    "                        wait_time = 2 ** attempt  # 指数退避\n",
    "                        print(f\"Embedding failed: {e}. Retrying in {wait_time}s...\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        raise\n",
    "            \n",
    "            # 定期保存检查点\n",
    "            if (i + 1) % 10 == 0:\n",
    "                np.savez(checkpoint_file, embeddings=np.array(all_embeddings, dtype=object))\n",
    "            \n",
    "            elapsed = time.time() - start\n",
    "            done = len(all_embeddings)\n",
    "            rate = done / elapsed if elapsed > 0 else 0.0\n",
    "            pbar.set_postfix({\"docs/sec\": f\"{rate:.1f}\"})\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Operation interrupted. Saving progress...\")\n",
    "    finally:\n",
    "        if all_embeddings:\n",
    "            np.savez(checkpoint_file, embeddings=np.array(all_embeddings, dtype=object))\n",
    "    \n",
    "    # 构建并保存 FAISS 索引\n",
    "    docs = [Document(page_content=text, metadata={\"source\": id_})\n",
    "            for text, id_ in zip(passages[:len(all_embeddings)], ids[:len(all_embeddings)])]\n",
    "    \n",
    "    db = FAISS.from_embeddings(all_embeddings, docs)\n",
    "    db.save_local(INDEX_DIR)\n",
    "    print(f\"Index saved to {INDEX_DIR}\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e84414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index():\n",
    "    \"\"\"\n",
    "    加载已有索引；若不存在则构建带速率的索引\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(INDEX_DIR) or not os.path.exists(os.path.join(INDEX_DIR, \"index.faiss\")):\n",
    "        print(\"Index not found, building new index...\")\n",
    "        return build_index_with_speed(batch_size=200)\n",
    "    print(f\"Loading existing index from {INDEX_DIR}\")\n",
    "    embeddings = OpenAIEmbeddings(model=settings.EMBEDDING_MODEL)\n",
    "    return FAISS.load_local(INDEX_DIR, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d210be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(dev_ds, db, top_k=TOP_K, batch_size=32):\n",
    "    \"\"\"\n",
    "    针对 validation split，计算 Hit@k 与 MRR@k，使用批处理提高效率\n",
    "    \"\"\"\n",
    "    # 构建 qrels：query_id -> 正例 passage_text 集合（使用集合加速查找）\n",
    "    qrels = {}\n",
    "    for item in tqdm(dev_ds, desc=\"Building qrels\"):\n",
    "        qid = item[\"query_id\"]\n",
    "        p = item[\"passages\"]\n",
    "        positives = set(txt for txt, sel in zip(p[\"passage_text\"], p[\"is_selected\"]) if sel == 1)\n",
    "        qrels[qid] = positives\n",
    "    \n",
    "    results = {\"hits\": [], \"rrs\": []}\n",
    "    \n",
    "    # 批处理评估\n",
    "    for i in range(0, len(dev_ds), batch_size):\n",
    "        batch = dev_ds[i:i+batch_size]\n",
    "        queries = [item[\"query\"] for item in batch]\n",
    "        qids = [item[\"query_id\"] for item in batch]\n",
    "        \n",
    "        # 批量检索\n",
    "        batch_results = []\n",
    "        for query in queries:\n",
    "            retrieved = db.similarity_search(query, k=top_k)\n",
    "            batch_results.append([doc.page_content for doc in retrieved])\n",
    "        \n",
    "        for qid, texts in zip(qids, batch_results):\n",
    "            positives = qrels.get(qid, set())\n",
    "            \n",
    "            # 精确匹配而非子字符串匹配\n",
    "            hit = int(any(txt in positives for txt in texts))\n",
    "            results[\"hits\"].append(hit)\n",
    "            \n",
    "            # MRR@k\n",
    "            rr = 0\n",
    "            for rank, txt in enumerate(texts):\n",
    "                if txt in positives:\n",
    "                    rr = 1.0 / (rank + 1)\n",
    "                    break\n",
    "            results[\"rrs\"].append(rr)\n",
    "    \n",
    "    return np.mean(results[\"hits\"]), np.mean(results[\"rrs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c99215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index not found, building new index...\n",
      "Loading MS MARCO v1.1 train split passages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting passages: 100%|██████████| 82326/82326 [00:06<00:00, 12470.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 626907 unique passages.\n",
      "Error loading checkpoint: No data left in file. Starting from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches:   1%|          | 18/3135 [00:41<2:00:52,  2.33s/it, docs/sec=88.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation interrupted. Saving progress...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     db \u001b[38;5;241m=\u001b[39m \u001b[43mload_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading MS MARCO v1.1 validation split for evaluation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     dev_ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mms_marco\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36mload_index\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(INDEX_DIR) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(INDEX_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex.faiss\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex not found, building new index...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_index_with_speed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading existing index from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mINDEX_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(model\u001b[38;5;241m=\u001b[39msettings\u001b[38;5;241m.\u001b[39mEMBEDDING_MODEL)\n",
      "Cell \u001b[1;32mIn[2], line 83\u001b[0m, in \u001b[0;36mbuild_index_with_speed\u001b[1;34m(batch_size, cache_dir)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# 构建并保存 FAISS 索引\u001b[39;00m\n\u001b[0;32m     80\u001b[0m docs \u001b[38;5;241m=\u001b[39m [Document(page_content\u001b[38;5;241m=\u001b[39mtext, metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: id_})\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m text, id_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(passages[:\u001b[38;5;28mlen\u001b[39m(all_embeddings)], ids[:\u001b[38;5;28mlen\u001b[39m(all_embeddings)])]\n\u001b[1;32m---> 83\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m db\u001b[38;5;241m.\u001b[39msave_local(INDEX_DIR)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mINDEX_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\AriZu\\.conda\\envs\\train\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1119\u001b[0m, in \u001b[0;36mFAISS.from_embeddings\u001b[1;34m(cls, text_embeddings, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_embeddings\u001b[39m(\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1098\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \n\u001b[0;32m   1101\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_embeddings(text_embedding_pairs, embeddings)\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1119\u001b[0m     texts, embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtext_embeddings)\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m   1121\u001b[0m         \u001b[38;5;28mlist\u001b[39m(texts),\n\u001b[0;32m   1122\u001b[0m         \u001b[38;5;28mlist\u001b[39m(embeddings),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1126\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1127\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    db = load_index()\n",
    "    print(\"Loading MS MARCO v1.1 validation split for evaluation...\")\n",
    "    dev_ds = load_dataset(\"ms_marco\", \"v1.1\", split=\"validation\")\n",
    "    hit10, mrr10 = evaluate_retrieval(dev_ds, db)\n",
    "    print(f\"MS MARCO v1.1 Retrieval => Hit@{TOP_K}: {hit10:.4f}, MRR@{TOP_K}: {mrr10:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
